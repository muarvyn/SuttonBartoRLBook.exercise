{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.load(\"Q.npy\")\n",
    "C = np.load(\"C.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGreedyPolicy(Q):\n",
    "    return np.argmax(Q,axis=-1)\n",
    "\n",
    "def innerMCControl(episode, Q, C, t_policy, getBPolicy):\n",
    "    b_policy = getBPolicy(Q)\n",
    "    G = 0.0\n",
    "    W = 1.0\n",
    "    converged = True\n",
    "    step = len(episode)-1\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = gamma*G + reward\n",
    "        C[state][action] += W\n",
    "        q = Q[state][action]\n",
    "        Q[state][action] += W/C[state][action] * (G-Q[state][action])\n",
    "        converged = converged and abs(q - Q[state][action]) < CONVERGENCE_ERROR\n",
    "        t_policy[state] = np.argmax(Q[state])\n",
    "        if t_policy[state] != action: break\n",
    "        W *= 1.0/b_policy[state][action]\n",
    "        step -= 1\n",
    "    \n",
    "    print(\"\\repisode length:{:7}; steps used:{:3}\".format(len(episode),len(episode)-step), \n",
    "          end='', flush=True)\n",
    "    return converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCControl(episodes,C,getBPolicy,QShape):\n",
    "    C = np.zeros(dtype=np.float, shape=episodes.QShape)\n",
    "    t_policy = getGreedyPolicy(episodes.Q)\n",
    "    for episode in episodes:\n",
    "        innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of RaceTrack environment starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VELOCITY = 4\n",
    "MIN_VELOCITY = 0\n",
    "TRACK_TAG = 1\n",
    "\n",
    "## Track grid dimensions are [bottom-to-top, left-to-right]\n",
    "RT_grid = [3,6,3,\n",
    "           2,7,7,\n",
    "           1,8,8,\n",
    "           0,9,7,\n",
    "           0,10,1,\n",
    "           0,17,2,\n",
    "           1,16,1,\n",
    "           2,15,2,\n",
    "           3,14,1]\n",
    "START_TAG = 2\n",
    "start_line = (0,3,1,9)\n",
    "FINISH_TAG = 3\n",
    "finish_line = (26,16,32,17)\n",
    "track_data = np.array(RT_grid).reshape((-1,3))\n",
    "REWARD = -1\n",
    "CONVERGENCE_ERROR = 0.02\n",
    "ACCELERATION =  [[1,-1],  [1,0],  [1,1],\n",
    "                 [0,-1],  [0,0],  [0,1],\n",
    "                [-1,-1], [-1,0], [-1,1]]\n",
    "ACTIONS_NUM = len(ACCELERATION)\n",
    "\n",
    "### Building race track ###\n",
    "# Determine extents of race track grid.\n",
    "track = np.zeros(shape=(np.sum(track_data[:,-1]), \n",
    "                        max(np.sum(track_data[:,:-1], axis=1))),\n",
    "                dtype = np.uint8)\n",
    "print(track.shape)\n",
    "row=0\n",
    "# Fill the race track grid with designed designation marks\n",
    "for start,length,repeat in track_data:\n",
    "    for i in range(repeat):\n",
    "        track[row, start:start+length] = TRACK_TAG\n",
    "        row += 1\n",
    "track[start_line[0]:start_line[2],start_line[1]:start_line[3]] = START_TAG\n",
    "track[finish_line[0]:finish_line[2],finish_line[1]:finish_line[3]] = FINISH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_getStartPosition():\n",
    "    return (0,np.random.randint(start_line[1], start_line[3]),0,0)\n",
    "\n",
    "def rt_transition(track, state, action, getStartPosition=rt_getStartPosition):\n",
    "    #next_velocity = np.vectorize(lambda x: min(x,MAX_VELOCITY))(state[1]+action)\n",
    "    accel = ACCELERATION[action]\n",
    "    next_velocity = np.clip([state[2]+accel[0],state[3]+accel[1]], \n",
    "                            MIN_VELOCITY, MAX_VELOCITY)\n",
    "    if (next_velocity == [0,0]).all():\n",
    "        next_velocity = [state[2],state[3]]\n",
    "    position = [state[0],state[1]]\n",
    "    next_position = position + next_velocity\n",
    "    disp = next_velocity\n",
    "    #intersection_row = (disp[0]/disp[1]) * (track.shape[1]-position[1]) + position[0]\n",
    "    is_finished = next_position[1] >= track.shape[1] and \\\n",
    "        26.0 <= (disp[0]/disp[1]) * (track.shape[1]-position[1]) + position[0] < 32.0\n",
    "    \n",
    "    if is_finished: return (True,getStartPosition())\n",
    "    \n",
    "    next_position = tuple(next_position)\n",
    "    if next_position in np.ndindex(track.shape) and \\\n",
    "        track[next_position] > 0:\n",
    "        next_state = next_position + tuple(next_velocity)\n",
    "    else:\n",
    "        next_state = getStartPosition()\n",
    "        \n",
    "    return (False,next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBPolicy(Q, epsilon):\n",
    "    eps_policy = np.ones(dtype=np.float, shape=Q.shape) * epsilon/ACTIONS_NUM\n",
    "    for state in np.ndindex(Q.shape[:-1]):\n",
    "        action = np.argmax(Q[state])\n",
    "        eps_policy[state][action] += 1.0-epsilon\n",
    "    return eps_policy\n",
    "\n",
    "def getBPolicyLog(Q, epsilon, step):\n",
    "    e = min(1.0, epsilon/np.log(max(2,step/100)))\n",
    "    eps_policy = np.ones(dtype=np.float, shape=Q.shape) * e/ACTIONS_NUM\n",
    "    for state in np.ndindex(Q.shape[:-1]):\n",
    "        action = np.argmax(Q[state])\n",
    "        eps_policy[state][action] += 1.0-e\n",
    "    return eps_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disp[1]/disp[0] == (track.shape[1]-position[1])/(track_row-position[0])\n",
    "track_row = (disp[0]/disp[1]) * (track.shape[1]-position[1]) + position[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(policy, state):\n",
    "    return (np.nonzero(np.random.multinomial(1, policy[state]))[0])[0]\n",
    "\n",
    "def rt_generateEpisode(b_policy, getAction):\n",
    "    state = rt_getStartPosition()\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = getAction(b_policy, state)\n",
    "        episode.append((state, action, reward))\n",
    "        is_finished, next_state = rt_transition(track, state, action)\n",
    "        if is_finished: break\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n",
    "\n",
    "class SequenceGenerator:\n",
    "    def __init__(self, getAction, getStartState, getTransition, episode_imax=1):\n",
    "        self.episode_imax = episode_imax\n",
    "        self.episode_i=1\n",
    "        self.get_action = getAction\n",
    "        self.get_start_state = getStartState\n",
    "        self.state = self.get_start_state(self.episode_i)\n",
    "        self.get_transition = getTransition\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.episode_i=1\n",
    "        self.state = self.get_start_state(self.episode_i)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.episode_imax > 0 and self.episode_i > self.episode_imax:\n",
    "            raise StopIteration\n",
    "\n",
    "        action = self.get_action(self.state, self.episode_i)\n",
    "        keep_state = self.state\n",
    "        is_terminal, self.state, reward = self.get_transition(keep_state, action)\n",
    "        self.episode_i += int(is_terminal)\n",
    "        return keep_state, is_terminal, self.state, action, reward\n",
    "    \n",
    "REWARD_I = 4\n",
    "STATE_I = 0\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, Q, Epsilon=0.1):\n",
    "        self.Q = Q;\n",
    "        self.epsilon = Epsilon\n",
    "        \n",
    "    def __call__(self, state, episode_i=1):\n",
    "        q = self.Q[state]\n",
    "        if np.random.rand(1)[0] < self.epsilon:\n",
    "            return np.random.randint(0,len(q))\n",
    "        return np.argmax(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of n-step SARSA method starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "QShape = track.shape + (MAX_VELOCITY+1, MAX_VELOCITY+1, ACTIONS_NUM)\n",
    "Q = np.zeros(QShape) #(np.random.random(QShape)-0.5)*0.1\n",
    "n = 10\n",
    "gamma_powered = [gamma**n for n in range(0,n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_iter = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.1), \n",
    "                                  lambda e: rt_getStartPosition(),\n",
    "                                  lambda s,a: rt_transition(track, s, a) + (REWARD,),\n",
    "                                  0\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for each episode:\n",
    "for episode_i in range(50000):\n",
    "    # Initialize and store S[0] != terminal\n",
    "    # Select and store an action A[0] ~ π(·|S[0])\n",
    "    step = next(sequence_iter)\n",
    "    state, is_terminal, next_state, action, reward = step\n",
    "    # T ← ∞\n",
    "    T = sys.maxsize\n",
    "    # Loop for t = 0, 1, 2, . . . :\n",
    "    t = 0\n",
    "    history = [step]\n",
    "\n",
    "    while True:\n",
    "        # If t < T , then:\n",
    "        if t < T:\n",
    "            # Take action A[t]\n",
    "            # Observe and store the next reward as R[t+1] and the next state as S[t+1]\n",
    "            # already done, when selected A[t]\n",
    "            # If S[t+1] is terminal, then:\n",
    "            if is_terminal:\n",
    "            # T ← t+1\n",
    "                T = t+1\n",
    "                print(\"\\rEpisode length is {:7}; tau={:7}\".format(T,tau), end='', flush=True)\n",
    "            # else:\n",
    "            else:\n",
    "                # Select and store an action A[t+1] ~ π(·|S[t+1])\n",
    "                step = next(sequence_iter)\n",
    "                # get S[t+1], is_term, S[t+2], A[t+1], R[t+2]\n",
    "                state, is_terminal, next_state, action, next_reward = step\n",
    "                history.append(step)\n",
    "\n",
    "        # τ ← t-n+1 (τ is the time whose estimate is being updated)\n",
    "        tau = t-n+1  # tau+n == t+1\n",
    "        # If τ ≥ 0:\n",
    "        if tau >= 0:\n",
    "            # G ← Sum(i, τ+1, min(τ+n,T))(γ^(i-τ-1)*R[i])\n",
    "            # min(τ+n,T) == min(n,T-τ) + τ\n",
    "            # in the following i == j+τ+1; history[i] containes R[i+1]\n",
    "            G = np.sum( [gamma_powered[j]*history[tau+j][REWARD_I] \n",
    "                         for j in range(0,min(n,T-tau))])\n",
    "            # If τ + n < T , then G ← G + γ^n * Q(S[τ+n], A[τ+n])\n",
    "            if tau+n < T:\n",
    "                G = G + gamma_powered[n] * Q[state][action]\n",
    "            # Q(S[τ], A[τ]) ← Q(S[τ], A[τ]) + α*[G - Q(S[τ], A[τ])]\n",
    "            # If π is being learned, then ensure that π(·|S[τ]) is ε-greedy wrt Q\n",
    "            Stau, istrm, next_tau, Atau, Rtau = history[tau]\n",
    "            Q[Stau][Atau] = Q[Stau][Atau] + alpha*(G - Q[Stau][Atau])\n",
    "\n",
    "        # Until τ = T-1\n",
    "        if tau == T-1: break\n",
    "        t += 1\n",
    "print(\"\\nFinished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[20, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tau, t, T, len(history), is_terminal, state, ACCELERATION[action], next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Stau,Atau,G)\n",
    "Q[Stau][Atau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    s0, is_terminal, state, action, reward = next(sequence_iter)\n",
    "    print( s0, is_terminal, state, ACCELERATION[action], reward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Q-Sarsa-n\",Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Monte-Carlo method starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episodes:\n",
    "    def __init__(self, N, Q, track, getBPolicy):\n",
    "        self.samples_num = N\n",
    "        self.state_shape = Q.shape[:-1]\n",
    "        self.Q = Q\n",
    "        self.getBPolicy = getBPolicy\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.N = self.samples_num\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.N == 0: raise StopIteration\n",
    "        self.N -= 1\n",
    "        self.policy = self.getBPolicy(self.Q, self.samples_num-self.N)\n",
    "        return rt_generateEpisode(self.policy, getAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_run(N, Q, C):\n",
    "    episodes = Episodes(N, Q, track, lambda q,s: getBPolicy(q,10000.0/(20000.0+s)))\n",
    "                        #lambda q,s: rt_getBPolicy(q,0.5)) #lambda q,s: getBPolicyLog(q,0.7,s))\n",
    "    t_policy = getGreedyPolicy(episodes.Q)\n",
    "    conv_count = 0\n",
    "    for episode in episodes:\n",
    "        if innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy):\n",
    "            conv_count += 1\n",
    "        else: \n",
    "            conv_count = 0\n",
    "        if conv_count >= 500:\n",
    "            print(\"\\nConvergence reached.\")\n",
    "            break\n",
    "            \n",
    "    print(\"\\nEpisodes generated: {}\".format(episodes.samples_num - episodes.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QShape = track.shape + (MAX_VELOCITY+1, MAX_VELOCITY+1, ACTIONS_NUM)\n",
    "Q = (np.random.random(QShape)-0.5)*0.1 - 30.0\n",
    "C = np.zeros(dtype=np.float, shape=Q.shape)\n",
    "MC_run(40000, Q, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_run(50000, Q, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildEpisode(startPosition, policy):\n",
    "    state = startPosition\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        episode.append((state, action, reward))\n",
    "        is_finished, next_state = rt_transition(track, state, action, lambda:startPosition)\n",
    "        if is_finished or next_state==startPosition: break\n",
    "        state = next_state\n",
    "        \n",
    "    return episode\n",
    "\n",
    "def buildTrack(policy):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getGreedyPolicy(Q)\n",
    "episode = buildEpisode((0,5,0,0),p)\n",
    "last = episode[-1]\n",
    "episode.append((tuple(np.array(last[0][:2])+np.array(last[0][2:])+ACCELERATION[last[1]]) +\n",
    "               (0,0),0,len(episode)))\n",
    "\n",
    "trace = np.array([list(s[:2]) for s,a,r in episode])\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy:\n",
    "    def __init__(self, Q):\n",
    "        self.action = np.argmax(Q,axis=-1);\n",
    "        \n",
    "    def __call__(self, state, dummy):\n",
    "        return self.action[state]\n",
    "\n",
    "r_count = 0\n",
    "def restart(e):\n",
    "    global r_count\n",
    "    r_count += 1\n",
    "    return (0, 6, 0, 0)\n",
    "\n",
    "test_iter = SequenceGenerator(\n",
    "    GreedyPolicy(Q), \n",
    "    restart,\n",
    "    lambda s,a: rt_transition(track, s, a, getStartPosition = lambda : restart(0)) + (REWARD,),\n",
    "    1\n",
    "                             )\n",
    "trace=[]\n",
    "for s,trm,ns,a,r in test_iter:\n",
    "    trace.append((s,ACCELERATION[a]))\n",
    "    #trace.append(s[:2])\n",
    "    if trm or r_count > 2: break\n",
    "\n",
    "print(trace)\n",
    "trace = np.array(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCELERATION[GreedyPolicy(Q).action[0,3,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(num=None, figsize=(8,10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.pcolor(track, figure=f)\n",
    "plt.plot(trace[:,1], trace[:,0], figure=f, linewidth=4.0, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QShape = track.shape + (MAX_VELOCITY+1, MAX_VELOCITY+1, ACTIONS_NUM)\n",
    "Q = (np.random.random(QShape)-0.5)*0.1 - 30.0\n",
    "episodes = Episodes(200, Q, track, lambda q,s: getBPolicyLog(q,0.7,s))\n",
    "C = np.zeros(dtype=np.float, shape=episodes.Q.shape)\n",
    "t_policy = getGreedyPolicy(episodes.Q)\n",
    "conv_count = 0\n",
    "for episode in episodes:\n",
    "    if innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy):\n",
    "        conv_count += 1\n",
    "    else: \n",
    "        conv_count = 0\n",
    "    if conv_count > 100:\n",
    "        print(\"Convergence reached.\")\n",
    "print(\"\\nEpisodes generated: {}\".format(episodes.samples_num - episodes.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = Episodes(2000, episodes.Q, track, lambda Q,s: rt_getBPolicy(Q,s,0.5))\n",
    "for episode in episodes:\n",
    "    innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_getTargetPolicyAction(Q, state):\n",
    "    return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Q\",Q)\n",
    "np.save(\"C\",C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "getBPolicy = lambda Q: rt_getBPolicy(Q,epsilon)\n",
    "MCControl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ns = rt_transition(track,state,action)\n",
    "print(f,ns)\n",
    "[(ACCELERATION[a],rt_transition(track,ns,a)) for a in range(9)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
