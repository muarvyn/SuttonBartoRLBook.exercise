{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.load(\"Q.npy\")\n",
    "C = np.load(\"C.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original implementation of Monte-Carlo method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGreedyPolicy(Q):\n",
    "    return np.argmax(Q,axis=-1)\n",
    "\n",
    "def innerMCControl(episode, Q, C, t_policy, getBPolicy):\n",
    "    b_policy = getBPolicy(Q)\n",
    "    G = 0.0\n",
    "    W = 1.0\n",
    "    converged = True\n",
    "    step = len(episode)-1\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = gamma*G + reward\n",
    "        C[state][action] += W\n",
    "        q = Q[state][action]\n",
    "        Q[state][action] += W/C[state][action] * (G-Q[state][action])\n",
    "        converged = converged and abs(q - Q[state][action]) < CONVERGENCE_ERROR\n",
    "        t_policy[state] = np.argmax(Q[state])\n",
    "        if t_policy[state] != action: break\n",
    "        W *= 1.0/b_policy[state][action]\n",
    "        step -= 1\n",
    "    \n",
    "    print(\"\\repisode length:{:7}; steps used:{:3}\".format(len(episode),len(episode)-step), \n",
    "          end='', flush=True)\n",
    "    return converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCControl(episodes,C,getBPolicy,QShape):\n",
    "    C = np.zeros(dtype=np.float, shape=episodes.QShape)\n",
    "    t_policy = getGreedyPolicy(episodes.Q)\n",
    "    for episode in episodes:\n",
    "        innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(base, line):\n",
    "    return np.array([line[0]-base, line[1]-base])\n",
    "    \n",
    "def det(v1,v2):\n",
    "    return v1[0]*v2[1]-v1[1]*v2[0]\n",
    "\n",
    "def is_intersection(l1, l2):\n",
    "    return (det(*getVectors(l1[0],l2)) < 0) != (det(*getVectors(l1[1],l2)) < 0) \\\n",
    "           and (det(*getVectors(l2[0],l1)) < 0) != (det(*getVectors(l2[1],l1)) < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of RaceTrack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VELOCITY = 4\n",
    "MIN_VELOCITY = 0\n",
    "\n",
    "rt_contour_1 = [\n",
    "    [3,0],[3,3],[2,3],[2,10],[1,10],[1,18],[0,18],[0,28],[1,28],[1,29],[2,29],\n",
    "    [2,31],[3,31],[3,32],[17,32],[17,26],[10,26],[10,25],[9,25],[9,0],[3,0]\n",
    "]\n",
    "start_line_1 = (3,0,9,1)\n",
    "finish_line_1 = (16,26,17,32)\n",
    "ACCELERATION =  [[1,-1],  [1,0],  [1,1],\n",
    "                 [0,-1],  [0,0],  [0,1],\n",
    "                [-1,-1], [-1,0], [-1,1]]\n",
    "ACTIONS_NUM = len(ACCELERATION)\n",
    "\n",
    "REWARD = -1\n",
    "CONVERGENCE_ERROR = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_getStartPosition(start_line):\n",
    "    return (0,np.random.randint(start_line[0], start_line[2]),0,0)\n",
    "\n",
    "def is_finished(finish_line, position, next_position):\n",
    "    return is_intersection(np.array([(finish_line[0],finish_line[1]),\n",
    "                                     (finish_line[0],finish_line[3])]),\n",
    "                           np.array([np.array(position)+0.5, \n",
    "                                     np.array(next_position)+0.5]))\n",
    "\n",
    "def is_runout(contour, position, next_position):\n",
    "    it = iter(contour)\n",
    "    p1 = next(it)\n",
    "    for p2 in it:\n",
    "        if is_intersection(np.array([p1,p2]), \n",
    "                           np.array([np.array(position)+0.5, \n",
    "                                     np.array(next_position)+0.5])):\n",
    "            return True\n",
    "        p1 = p2\n",
    "    return False\n",
    "\n",
    "def rt_getTransition(track_contour, state, action, finish_line, getStartPosition=lambda: rt_getStartPosition(start_line_1)):\n",
    "    accel = ACCELERATION[action]\n",
    "    next_velocity = np.clip([state[2]+accel[0],state[3]+accel[1]], \n",
    "                            MIN_VELOCITY, MAX_VELOCITY)\n",
    "    if (next_velocity == [0,0]).all():\n",
    "        next_velocity = [state[2],state[3]]\n",
    "    position = [state[0],state[1]]\n",
    "    next_position = position + next_velocity\n",
    "\n",
    "    if is_finished(finish_line, position, next_position):\n",
    "        return (True,getStartPosition())\n",
    "\n",
    "    if not is_runout(track_contour, position, next_position):\n",
    "        next_state = tuple(next_position) + tuple(next_velocity)\n",
    "    else:\n",
    "        next_state = getStartPosition()\n",
    "        \n",
    "    return (False,next_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of RaceTrack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_intersection(np.array([(finish_line_1[0],finish_line_1[1]),\n",
    "                                     (finish_line_1[2],finish_line_1[1])]),\n",
    "                           np.array([np.array([4,26])+0.5, \n",
    "                                     np.array([18,30])+0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(finish_line_1[0],finish_line_1[1]),\n",
    "                                     (finish_line_1[0],finish_line_1[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_getTransition(rt_contour_1, (12,30,3,1), 8, finish_line_1, \n",
    "                 getStartPosition=lambda: rt_getStartPosition(start_line_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(contour)\n",
    "p1 = next(it)\n",
    "for p2 in it:\n",
    "    print((p1,p2))\n",
    "    if is_intersection(np.array([p1,p2]), \n",
    "                       np.array([np.array(position)+0.5, \n",
    "                                 np.array(next_position)+0.5])):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test line intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.array([(3.5,0.5),(2.5,5.5)])\n",
    "l2 = np.array([(2,8),(1,8)])\n",
    "l3 = np.array([(4,2.5),(4,4)])\n",
    "l4 = np.array([(5,1.5),(5,3)])\n",
    "\n",
    "plt.plot([0],[0],'bo', \n",
    "         l1[:,0], l1[:,1], 'r', \n",
    "         l2[:,0], l2[:,1], 'y', \n",
    "         l3[:,0], l3[:,1], 'm',\n",
    "         l4[:,0], l4[:,1], 'g',)\n",
    "\n",
    "# l1 and l2 intersect\n",
    "print(det(*getVectors(l1[0],l2)), det(*getVectors(l1[1],l2))) \n",
    "print(det(*getVectors(l2[0],l1)), det(*getVectors(l2[1],l1)))\n",
    "# l3 and l2 do not intersect\n",
    "print(det(*getVectors(l3[0],l2)), det(*getVectors(l3[1],l2)))\n",
    "print(det(*getVectors(l2[0],l3)), det(*getVectors(l2[1],l3)))\n",
    "# l1 and l3 intersect\n",
    "print(det(*getVectors(l1[0],l3)), det(*getVectors(l1[1],l3)))\n",
    "print(det(*getVectors(l3[0],l1)), det(*getVectors(l3[1],l1)))\n",
    "\n",
    "# l4 does not intersect with the rest\n",
    "print(\"l4 intersections:\")\n",
    "print(det(*getVectors(l4[0],l3)), det(*getVectors(l4[1],l3)))\n",
    "print(det(*getVectors(l4[0],l1)), det(*getVectors(l4[1],l1)))\n",
    "print(det(*getVectors(l4[0],l2)), det(*getVectors(l4[1],l2)))\n",
    "print(det(*getVectors(l1[0],l4)), det(*getVectors(l1[1],l4))) \n",
    "print(det(*getVectors(l2[0],l4)), det(*getVectors(l2[1],l4)))\n",
    "print(det(*getVectors(l3[0],l4)), det(*getVectors(l3[1],l4)))\n",
    "\n",
    "print((det(*getVectors(l1[0],l2)) < 0) != (det(*getVectors(l1[1],l2)) < 0) \\\n",
    "           and (det(*getVectors(l2[0],l1)) < 0) != (det(*getVectors(l2[1],l1)) < 0))\n",
    "\n",
    "[is_intersection(l1,l2), is_intersection(l1,l3), is_intersection(l1,l4), \n",
    " is_intersection(l2,l3), is_intersection(l2,l4), is_intersection(l3,l4),\n",
    " is_intersection(l2,l1), is_intersection(l3,l1), is_intersection(l4,l1),\n",
    " is_intersection(l3,l2), is_intersection(l4,l2), is_intersection(l4,l3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det(*getVectors((2,8),l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original implementation of Îµ-greedy policy (for history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBPolicy(Q, epsilon):\n",
    "    eps_policy = np.ones(dtype=np.float, shape=Q.shape) * epsilon/ACTIONS_NUM\n",
    "    for state in np.ndindex(Q.shape[:-1]):\n",
    "        action = np.argmax(Q[state])\n",
    "        eps_policy[state][action] += 1.0-epsilon\n",
    "    return eps_policy\n",
    "\n",
    "def getBPolicyLog(Q, epsilon, step):\n",
    "    e = min(1.0, epsilon/np.log(max(2,step/100)))\n",
    "    eps_policy = np.ones(dtype=np.float, shape=Q.shape) * e/ACTIONS_NUM\n",
    "    for state in np.ndindex(Q.shape[:-1]):\n",
    "        action = np.argmax(Q[state])\n",
    "        eps_policy[state][action] += 1.0-e\n",
    "    return eps_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Racetrack learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import SeqGen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = SeqGen.SequenceGeneratorPlus(\n",
    "                                SeqGen.EpsilonGreedyPolicy(Q, 0.05),\n",
    "                                rt_getStartPosition,\n",
    "                                lambda s,a: rt_getTransition(track, s, a) + (REWARD,),\n",
    "                                10000\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_run(N, Q, C):\n",
    "    episodes = Episodes(N, Q, track, lambda q,s: getBPolicy(q,10000.0/(20000.0+s)))\n",
    "                        #lambda q,s: rt_getBPolicy(q,0.5)) #lambda q,s: getBPolicyLog(q,0.7,s))\n",
    "    t_policy = getGreedyPolicy(episodes.Q)\n",
    "    conv_count = 0\n",
    "    for episode in episodes:\n",
    "        if innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy):\n",
    "            conv_count += 1\n",
    "        else: \n",
    "            conv_count = 0\n",
    "        if conv_count >= 500:\n",
    "            print(\"\\nConvergence reached.\")\n",
    "            break\n",
    "            \n",
    "    print(\"\\nEpisodes generated: {}\".format(episodes.samples_num - episodes.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QShape = track.shape + (MAX_VELOCITY+1, MAX_VELOCITY+1, ACTIONS_NUM)\n",
    "Q = (np.random.random(QShape)-0.5)*0.1 - 30.0\n",
    "C = np.zeros(dtype=np.float, shape=Q.shape)\n",
    "MC_run(40000, Q, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_run(50000, Q, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildEpisode(startPosition, policy):\n",
    "    state = startPosition\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        episode.append((state, action, reward))\n",
    "        is_finished, next_state = rt_getTransition(track, state, action, lambda:startPosition)\n",
    "        if is_finished or next_state==startPosition: break\n",
    "        state = next_state\n",
    "        \n",
    "    return episode\n",
    "\n",
    "def buildTrack(policy):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getGreedyPolicy(Q)\n",
    "episode = buildEpisode((0,5,0,0),p)\n",
    "last = episode[-1]\n",
    "episode.append((tuple(np.array(last[0][:2])+np.array(last[0][2:])+ACCELERATION[last[1]]) +\n",
    "               (0,0),0,len(episode)))\n",
    "\n",
    "trace = np.array([list(s[:2]) for s,a,r in episode])\n",
    "episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the optimal paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImmutableGreedyPolicy:\n",
    "    def __init__(self, Q):\n",
    "        self.action = np.argmax(Q,axis=-1);\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        return self.action[state]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = 3\n",
    "def startPosition():\n",
    "    global epi\n",
    "    return (0,epi,0,0)\n",
    "\n",
    "\n",
    "test_gen = SeqGen.SequenceGeneratorPlus(\n",
    "    ImmutableGreedyPolicy(Q), \n",
    "    startPosition,\n",
    "    lambda s,a: rt_getTransition(track, s, a, getStartPosition = lambda : (0,7,0,0)) + (REWARD,),\n",
    "    episodes_max = 6,\n",
    "    episode_maxlen = 20\n",
    "    )\n",
    "\n",
    "traces = []\n",
    "episode = []\n",
    "for state, is_terminal, next_state, action, reward in test_gen:\n",
    "    episode.append(state[0:2])\n",
    "    if is_terminal:\n",
    "        traces.append(np.array(episode))\n",
    "        episode = []\n",
    "        epi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(num=None, figsize=(8,10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.pcolor(track, figure=f)\n",
    "cols = ['red','yellow','black','green','white','blue']\n",
    "i = 0\n",
    "for trace in traces:\n",
    "    plt.plot(trace[:,1], trace[:,0], figure=f, linewidth=4.0, color=cols[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QShape = track.shape + (MAX_VELOCITY+1, MAX_VELOCITY+1, ACTIONS_NUM)\n",
    "Q = (np.random.random(QShape)-0.5)*0.1 - 30.0\n",
    "episodes = Episodes(200, Q, track, lambda q,s: getBPolicyLog(q,0.7,s))\n",
    "C = np.zeros(dtype=np.float, shape=episodes.Q.shape)\n",
    "t_policy = getGreedyPolicy(episodes.Q)\n",
    "conv_count = 0\n",
    "for episode in episodes:\n",
    "    if innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy):\n",
    "        conv_count += 1\n",
    "    else: \n",
    "        conv_count = 0\n",
    "    if conv_count > 100:\n",
    "        print(\"Convergence reached.\")\n",
    "print(\"\\nEpisodes generated: {}\".format(episodes.samples_num - episodes.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = Episodes(2000, episodes.Q, track, lambda Q,s: rt_getBPolicy(Q,s,0.5))\n",
    "for episode in episodes:\n",
    "    innerMCControl(episode, episodes.Q, C, t_policy, lambda Q: episodes.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_getTargetPolicyAction(Q, state):\n",
    "    return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Q\",Q)\n",
    "np.save(\"C\",C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "getBPolicy = lambda Q: rt_getBPolicy(Q,epsilon)\n",
    "MCControl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ns = rt_getTransition(track,state,action)\n",
    "print(f,ns)\n",
    "[(ACCELERATION[a],rt_getTransition(track,ns,a)) for a in range(9)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
