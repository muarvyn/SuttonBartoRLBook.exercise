{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s, a) and Model(s, a) for all s ∈ S and a ∈ A(s)\n",
    "def DynaQ(Q, ModelReward, ModelState1, Sequence, n, alpha, gamma, UNOBSERVED):\n",
    "    # Loop forever:\n",
    "    for state, is_terminal, next_state, action, reward in Sequence:\n",
    "        # (a) S ← current (nonterminal) state\n",
    "        # (b) A ← ε-greedy(S, Q)\n",
    "        # (c) Take action A; observe resultant reward, R, and state, S`\n",
    "        state, is_terminal, next_state, action, reward = next(Sequence)\n",
    "        # (d) Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "        Q[state][action] = Q[state][action] + alpha*( reward + gamma*max(Q[state]) - Q[state][action])\n",
    "        # (e) Model(S, A) ← R, S` (assuming deterministic environment)\n",
    "        ModelReward[state][action] = reward\n",
    "        ModelState1[state][action] = next_state\n",
    "        # (f) Loop repeat n times:\n",
    "        observed = np.transpose(np.nonzero((ModelState1 != UNOBSERVED).all(axis=-1)))\n",
    "        for sample in np.random.randint(len(observed), size=n):\n",
    "            #   S ← random previously observed state\n",
    "            #   A ← random action previously taken in S\n",
    "            SA = tuple(observed[sample])\n",
    "            ### S = SA[:-1]\n",
    "            ### A = SA[-1]\n",
    "            #   R, S` ← Model(S, A)\n",
    "            R = ModelReward[SA]\n",
    "            S1 = tuple(ModelState1[SA])\n",
    "            #   Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "            Q[SA] = Q[SA] + alpha*( R + gamma*max(Q[S1]) - Q[SA])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGenerator:\n",
    "    def __init__(self, getAction, getStartState, getTransition, episode_imax=1, steps_max=0,\n",
    "                callBack=None):\n",
    "        self.episode_imax = episode_imax\n",
    "        self.get_action = getAction\n",
    "        self.get_start_state = getStartState\n",
    "        self.get_transition = getTransition\n",
    "        self.steps_max = steps_max\n",
    "        self.callback = callBack\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.episode_i=1\n",
    "        self.state = self.get_start_state(self.episode_i)\n",
    "        self.step_i = 1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.episode_imax > 0 and self.episode_i > self.episode_imax or \\\n",
    "            self.steps_max > 0 and self.step_i > self.steps_max:\n",
    "            raise StopIteration\n",
    "\n",
    "        action = self.get_action(self.state, self.episode_i)\n",
    "        keep_state = self.state\n",
    "        is_terminal, self.state, reward = self.get_transition(keep_state, action)\n",
    "        if self.callback: self.callback(self.episode_i, self.step_i, reward)\n",
    "        self.step_i += 1\n",
    "        self.episode_i += int(is_terminal)\n",
    "        return keep_state, is_terminal, self.state, action, reward\n",
    "\n",
    "REWARD_I = 4\n",
    "STATE_I = 0\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, Q, Epsilon=0.1):\n",
    "        self.Q = Q;\n",
    "        self.epsilon = Epsilon\n",
    "        \n",
    "    def __call__(self, state, episode_i=1):\n",
    "        q = self.Q[state]\n",
    "        if np.random.rand(1)[0] < self.epsilon:\n",
    "            return np.random.randint(0,len(q))\n",
    "        return np.argmax(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Maze Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_shape = (6,9,)\n",
    "maze_obstacles_1 = [[2,1,3,9]]\n",
    "maze_obstacles_2 = [[2,1,3,8]]\n",
    "OBSTACLE_TAG = 0\n",
    "START_TAG = 2\n",
    "start_cell = (0,3)\n",
    "TARGET_TAG = 3\n",
    "target_cell = (5,8)\n",
    "\n",
    "def buildMaze(shape, obstacles):\n",
    "    maze = np.ones(shape, dtype = np.uint8)\n",
    "    for begin_row, begin_column, end_row, end_column in obstacles:\n",
    "        for r in range(begin_row, end_row):\n",
    "            for c in range(begin_column, end_column):\n",
    "                maze[r,c] = OBSTACLE_TAG\n",
    "    return maze\n",
    "\n",
    "maze1 = buildMaze(maze_shape, maze_obstacles_1)\n",
    "maze2 = buildMaze(maze_shape, maze_obstacles_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions legend: Up, Right, Down, Left\n",
    "ACTIONS = np.array([[1,0], [0,1], [-1,0], [0,-1]])\n",
    "ACTIONS_NUM = len(ACTIONS)\n",
    "NO_REWARD = 0\n",
    "REWARD = 1\n",
    "UNOBSERVED = (-1,-1,)\n",
    "shape = maze_shape + (ACTIONS_NUM,)\n",
    "#Q = np.zeros(shape)\n",
    "Q = (np.random.random(shape)-0.5)*0.01\n",
    "#dt = np.dtype([('reward', float, 1),('state', np.int8, 2)])\n",
    "#Model = np.broadcast_to(np.array((0,UNOBSERVED,), dtype = dt), shape)\n",
    "ModelReward = np.zeros(shape)\n",
    "ModelState1 = np.full(shape+(2,),-1)\n",
    "#def mazeGetStartState():\n",
    "#    return start_cell\n",
    "\n",
    "def maze_getTransition(maze, state, action):\n",
    "    next_state = tuple(np.array(list(state)) + ACTIONS[action])\n",
    "    if not (next_state in np.ndindex(maze.shape) and maze[next_state] > 0):\n",
    "        return (False, state, NO_REWARD)\n",
    "    elif next_state == target_cell:\n",
    "        return (True, start_cell, REWARD)\n",
    "\n",
    "    return (False, next_state, NO_REWARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.95\n",
    "n = 5\n",
    "TotalReward = 0.0\n",
    "Reward = [0.0]\n",
    "\n",
    "def callback(e,s,r):\n",
    "    global TotalReward\n",
    "    TotalReward += r\n",
    "    if s % 100 == 0: Reward.append(TotalReward)\n",
    "\n",
    "gen = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.4),\n",
    "                        lambda e: start_cell,\n",
    "                        lambda s,a: maze_getTransition(maze1, s, a),\n",
    "                        0,\n",
    "                        3000,\n",
    "                        callback\n",
    "                       )\n",
    "sequence = iter(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DynaQ(Q, ModelReward, ModelState1, sequence, n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = np.transpose(np.nonzero((ModelState1 != UNOBSERVED).all(axis=-1)))\n",
    "len(observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, is_terminal, next_state, action, reward = next(sequence)\n",
    "# (d) Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "Q[state][action] = Q[state][action] + alpha*( reward + gamma*max(Q[state]) - Q[state][action])\n",
    "# (e) Model(S, A) ← R, S` (assuming deterministic environment)\n",
    "ModelReward[state][action] = reward\n",
    "ModelState1[state][action] = next_state\n",
    "# (f) Loop repeat n times:\n",
    "observed = np.transpose(np.nonzero((ModelState1 != UNOBSERVED).all(axis=-1)))\n",
    "len(observed), state, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in np.random.randint(len(observed), size=n):\n",
    "    #   S ← random previously observed state\n",
    "    #   A ← random action previously taken in S\n",
    "    SA = tuple(observed[sample])\n",
    "    ### S = SA[:-1]\n",
    "    ### A = SA[-1]\n",
    "    #   R, S` ← Model(S, A)\n",
    "    R = ModelReward[SA]\n",
    "    S1 = tuple(ModelState1[SA])\n",
    "    #   Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "    Q[SA] = Q[SA] + alpha*( R + gamma*max(Q[S1]) - Q[SA])\n",
    "    print(SA,S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = maze1\n",
    "gen = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.4), \n",
    "                        lambda e: start_cell,\n",
    "                        lambda s,a: maze_getTransition(maze, s, a),\n",
    "                        1\n",
    "                       )\n",
    "sequence = iter(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[(1,66),(77,-33)],\n",
    "              [(-3,2),(77,9)],\n",
    "              [(0,31),(1,45)]])\n",
    "#np.transpose(np.nonzero(x[...,0] == 1))\n",
    "#(x == [-3,2]).all(axis = -1)\n",
    "np.transpose(np.nonzero((x == [-3,2]).all(axis = -1)))\n",
    "#x[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "l = list(itertools.repeat((-1,-1,),10))\n",
    "a = np.array(l)\n",
    "b = a.reshape((2,5,2))\n",
    "(b == (-1,-1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
