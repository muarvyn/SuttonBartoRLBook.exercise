{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RG = np.random.RandomState(12)\n",
    "# Initialize Q(s, a) and Model(s, a) for all s ∈ S and a ∈ A(s)\n",
    "def DynaQPlus(Q, PlanningQ, ModelReward, ModelState1, Sequence, n, alpha, gamma, UNOBSERVED):\n",
    "    global RG\n",
    "    # Loop forever:\n",
    "        # (a) S ← current (nonterminal) state\n",
    "        # (b) A ← ε-greedy(S, Q)\n",
    "        # (c) Take action A; observe resultant reward, R, and state, S`\n",
    "    for state, is_terminal, next_state, action, reward in Sequence:\n",
    "        # (d) Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "        Q[state][action] = Q[state][action] + alpha*( \n",
    "            reward + gamma*max(Q[next_state]) - Q[state][action])\n",
    "        # (e) Model(S, A) ← R, S` (assuming deterministic environment)\n",
    "        ModelReward[state][action] = reward\n",
    "        ModelState1[state][action] = next_state\n",
    "        # (f) Loop repeat n times:\n",
    "        observed = np.transpose(np.nonzero((ModelState1 != UNOBSERVED).all(axis=-1)))\n",
    "        for sample in RG.randint(len(observed), size=n):\n",
    "            #   S ← random previously observed state\n",
    "            #   A ← random action previously taken in S\n",
    "            SA = tuple(observed[sample])\n",
    "            ### S = SA[:-1]\n",
    "            ### A = SA[-1]\n",
    "            #   R, S` ← Model(S, A)\n",
    "            R = ModelReward[SA]\n",
    "            S1 = tuple(ModelState1[SA])\n",
    "            #   Q(S, A) ← Q(S, A) + α*[R + γ*max(a)Q(S`, a) - Q(S, A)]\n",
    "            PlanningQ[SA] = PlanningQ[SA] + alpha*(\n",
    "                R + gamma*max(PlanningQ[S1]) - PlanningQ[SA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGeneratorPlus:\n",
    "    def __init__(self, getAction, getStartState, getTransition, episode_imax=1, steps_max=0,\n",
    "                callBack=None):\n",
    "        self.episode_imax = episode_imax\n",
    "        self.get_action = getAction\n",
    "        self.get_start_state = getStartState\n",
    "        self.get_transition = getTransition\n",
    "        self.steps_max = steps_max\n",
    "        self.callback = callBack\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.episode_i=1\n",
    "        self.state = self.get_start_state(self.episode_i)\n",
    "        self.step_i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.episode_imax > 0 and self.episode_i > self.episode_imax or \\\n",
    "            self.steps_max > 0 and self.step_i > self.steps_max:\n",
    "            raise StopIteration\n",
    "\n",
    "        action = self.get_action(self.state, self.episode_i)\n",
    "        keep_state = self.state\n",
    "        is_terminal, self.state, reward = self.get_transition(keep_state, action)\n",
    "        if self.callback:\n",
    "            self.callback(self, keep_state, is_terminal, self.state, action, reward)\n",
    "        self.step_i += 1\n",
    "        self.episode_i += int(is_terminal)\n",
    "        return keep_state, is_terminal, self.state, action, reward\n",
    "    \n",
    "REWARD_I = 4\n",
    "STATE_I = 0\n",
    "\n",
    "# TODO: rid of global variables\n",
    "random_generator = np.random.RandomState(11)\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, Q, Epsilon=0.1):\n",
    "        self.Q = Q;\n",
    "        self.epsilon = Epsilon\n",
    "\n",
    "    def __call__(self, state, episode_i=1):\n",
    "        global random_generator\n",
    "        q = self.Q[state]\n",
    "        if random_generator.rand(1)[0] < self.epsilon:\n",
    "            val = random_generator.randint(0,len(q))\n",
    "        else:\n",
    "            val = np.argmax(q)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalRewardCallback:\n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "        self.RewardSum = 0.0\n",
    "        self.TotalReward = []\n",
    "\n",
    "    def __call__(self, seq_gen, s0, isterm, s1, a, r):\n",
    "        self.RewardSum += r\n",
    "        if seq_gen.step_i % self.period == 0: \n",
    "            self.TotalReward.append(self.RewardSum)\n",
    "\n",
    "class TimestampCallback:\n",
    "    def __init__(self, planningQ, chainCallback=None):\n",
    "        self.planningQ = planningQ\n",
    "        self.chain = chainCallback\n",
    "\n",
    "    def __call__(self, seq_gen, s0, isterm, s1, a, r):\n",
    "        self.planningQ.timestamp[s0][a] = seq_gen.step_i\n",
    "        self.planningQ.step = seq_gen.step_i\n",
    "        self.chain(seq_gen, s0, isterm, s1, a, r)\n",
    "\n",
    "class SwitchMazeCallback:\n",
    "    def __init__(self, transitionFunc, switch_step, switch_maze, chainCallback=None):\n",
    "        self.chain = chainCallback\n",
    "        self.transition = transitionFunc\n",
    "        self.switch_step = switch_step\n",
    "        self.switch_maze = switch_maze\n",
    "\n",
    "    def __call__(self, seq_gen, s0, isterm, s1, a, r):\n",
    "        self.chain(seq_gen, s0, isterm, s1, a, r)\n",
    "        if seq_gen.step_i == self.switch_step: \n",
    "            self.transition.setMaze(self.switch_maze)\n",
    "\n",
    "class getMazeTransition:\n",
    "    def __init__(self, original_func, Maze):\n",
    "        self.func = original_func\n",
    "        self.maze = Maze\n",
    "\n",
    "    def __call__(self, state, action):\n",
    "        return self.func(self.maze, state, action)\n",
    "\n",
    "    def setMaze(self, Maze):\n",
    "        self.maze = Maze\n",
    "        \n",
    "class QPlus:\n",
    "    def __init__(self, Q, factor):\n",
    "        self.Q = Q\n",
    "        self.timestamp = np.zeros(shape = Q.shape, dtype=int)\n",
    "        self.factor = factor\n",
    "        self.step = 0\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.Q[key] + self.factor*np.sqrt(self.step - self.timestamp[key])\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.Q[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of Maze Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_shape = (6,9,)\n",
    "maze_obstacles_1 = [[2,1,3,9]]\n",
    "start_cell_1 = (0,3)\n",
    "target_cell_1 = (5,8)\n",
    "\n",
    "maze_obstacles_2 = [[2,1,3,8]]\n",
    "maze_obstacles_3 = [[2,2,5,3], [1,5,2,6], [3,7,6,8]]\n",
    "OBSTACLE_TAG = 0\n",
    "START_TAG = 2\n",
    "start_cell = (0,3)\n",
    "TARGET_TAG = 3\n",
    "target_cell = (5,8)\n",
    "\n",
    "def buildMaze(shape, obstacles, start, target):\n",
    "    maze = np.ones(shape, dtype = np.uint8)\n",
    "    for begin_row, begin_column, end_row, end_column in obstacles:\n",
    "        for r in range(begin_row, end_row):\n",
    "            for c in range(begin_column, end_column):\n",
    "                maze[r,c] = OBSTACLE_TAG\n",
    "    maze[start] = START_TAG\n",
    "    maze[target] = TARGET_TAG\n",
    "    return maze\n",
    "\n",
    "maze1 = buildMaze(maze_shape, maze_obstacles_1, start_cell, target_cell)\n",
    "maze2 = buildMaze(maze_shape, maze_obstacles_2, start_cell, target_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions legend: Up, Right, Down, Left\n",
    "ACTIONS = np.array([[1,0], [0,1], [-1,0], [0,-1]])\n",
    "ACTIONS_NUM = len(ACTIONS)\n",
    "NO_REWARD = 0\n",
    "REWARD = 1\n",
    "UNOBSERVED = (-1,-1,)\n",
    "shape = maze_shape + (ACTIONS_NUM,)\n",
    "\n",
    "def maze_getTransition(maze, getStartState, state, action):\n",
    "    next_state = tuple(np.array(list(state)) + ACTIONS[action])\n",
    "    if not (next_state in np.ndindex(maze.shape) and maze[next_state] > 0):\n",
    "        return (False, state, NO_REWARD)\n",
    "    elif maze[next_state] == TARGET_TAG:\n",
    "        return (True, getStartState(), REWARD)\n",
    "\n",
    "    return (False, next_state, NO_REWARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Maze Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "n = 40\n",
    "bonus_reward_factor = 2.e-4\n",
    "\n",
    "def setupSortcutMazeProblem():\n",
    "    Q = np.zeros(shape)\n",
    "    ModelReward = np.zeros(shape)\n",
    "    ModelState1 = np.full(shape+(2,),-1)\n",
    "    \n",
    "    maze_obstacles_1 = [[2,1,3,9]]\n",
    "    maze_obstacles_2 = [[2,1,3,8]]\n",
    "    start_cell = (0,3)\n",
    "    target_cell = (5,8)\n",
    "\n",
    "    maze1 = buildMaze(maze_shape, maze_obstacles_1, start_cell, target_cell)\n",
    "    maze2 = buildMaze(maze_shape, maze_obstacles_2, start_cell, target_cell)\n",
    "\n",
    "    getStartCell = lambda e=1: start_cell\n",
    "    total_reward_callback = TotalRewardCallback(100)\n",
    "\n",
    "    shortcut_maze_callback = SwitchMazeCallback(\n",
    "        getMazeTransition(lambda m,s,a: maze_getTransition(m, getStartCell, s, a), maze1),\n",
    "        3000,\n",
    "        maze2,\n",
    "        total_reward_callback\n",
    "        )\n",
    "    planning = QPlus(Q, bonus_reward_factor)\n",
    "    timestamp_callback = TimestampCallback(planning, shortcut_maze_callback)\n",
    "    return (Q, ModelReward, ModelState1, planning, timestamp_callback, total_reward_callback,\n",
    "            shortcut_maze_callback, getStartCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and run Dyna-Q\n",
    "Q, ModelReward, ModelState1, planning, timestamp_callback, DaynaQ_total_reward, \\\n",
    "    switch_callback, getStartCell = setupSortcutMazeProblem()\n",
    "\n",
    "random_generator.seed(2)\n",
    "gen = SequenceGeneratorPlus(EpsilonGreedyPolicy(Q, epsilon),\n",
    "                        getStartCell,\n",
    "                        switch_callback.transition,\n",
    "                        0,\n",
    "                        6000-1,\n",
    "                        switch_callback\n",
    "                       )\n",
    "\n",
    "RG = np.random.RandomState(12)\n",
    "DynaQPlus(Q, Q, ModelReward, ModelState1, iter(gen), n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and run Dyna-Q+\n",
    "Q, ModelReward, ModelState1, planning, timestamp_callback, DaynaQPlus_total_reward, \\\n",
    "    switch_callback, getStartCell = setupSortcutMazeProblem()\n",
    "\n",
    "random_generator.seed(2)\n",
    "gen = SequenceGeneratorPlus(EpsilonGreedyPolicy(Q, epsilon),\n",
    "                        getStartCell,\n",
    "                        switch_callback.transition,\n",
    "                        0,\n",
    "                        6000-1,\n",
    "                        timestamp_callback\n",
    "                       )\n",
    "\n",
    "# Dyna-Q+ specific:\n",
    "for i in np.ndindex(ModelState1.shape[:-1]):\n",
    "    ModelState1[i] = i[:-1]\n",
    "\n",
    "#sequence, test_sequence = itertools.tee(gen,2)\n",
    "RG = np.random.RandomState(12)\n",
    "DynaQPlus(Q, planning, ModelReward, ModelState1, iter(gen), n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,6000,100), DaynaQ_total_reward.TotalReward)\n",
    "plt.plot(range(0,6000,100), DaynaQPlus_total_reward.TotalReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking Maze Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "n = 40\n",
    "\n",
    "def setupBlockingMazeProblem():\n",
    "    Q = np.zeros(shape)\n",
    "    ModelReward = np.zeros(shape)\n",
    "    ModelState1 = np.full(shape+(2,),-1)\n",
    "    start_cell = (0,3)\n",
    "\n",
    "    blockMaze1 = buildMaze(maze_shape, [[2,0,3,8]], start_cell, (5,8))\n",
    "    blockMaze2 = buildMaze(maze_shape, [[2,1,3,9]], start_cell, (5,8))\n",
    "\n",
    "    getStartCell = lambda e=1: start_cell\n",
    "    total_reward_callback = TotalRewardCallback(100)\n",
    "\n",
    "    blocking_maze_callback = SwitchMazeCallback(\n",
    "        getMazeTransition(lambda m,s,a: maze_getTransition(m, getStartCell, s, a), blockMaze1),\n",
    "        1000,\n",
    "        blockMaze2,\n",
    "        total_reward_callback\n",
    "        )\n",
    "    planning = QPlus(Q, 2.e-4)\n",
    "    timestamp_callback = TimestampCallback(planning, blocking_maze_callback)\n",
    "    return (Q, ModelReward, ModelState1, planning, timestamp_callback, total_reward_callback,\n",
    "            blocking_maze_callback, getStartCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and run Dyna-Q\n",
    "Q, ModelReward, ModelState1, planning, timestamp_callback, DaynaQ_total_reward, \\\n",
    "    switch_callback, getStartCell = setupBlockingMazeProblem()\n",
    "\n",
    "random_generator.seed(2)\n",
    "gen = SequenceGeneratorPlus(EpsilonGreedyPolicy(Q, 0.1),\n",
    "                        getStartCell,\n",
    "                        switch_callback.transition,\n",
    "                        0,\n",
    "                        3000-1,\n",
    "                        switch_callback\n",
    "                       )\n",
    "\n",
    "RG = np.random.RandomState(12)\n",
    "DynaQPlus(Q, Q, ModelReward, ModelState1, iter(gen), n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and run Dyna-Q+\n",
    "Q, ModelReward, ModelState1, planning, timestamp_callback, DaynaQPlus_total_reward, \\\n",
    "    switch_callback, getStartCell = setupBlockingMazeProblem()\n",
    "\n",
    "random_generator.seed(2)\n",
    "gen = SequenceGeneratorPlus(EpsilonGreedyPolicy(Q, 0.1),\n",
    "                        getStartCell,\n",
    "                        switch_callback.transition,\n",
    "                        0,\n",
    "                        3000-1,\n",
    "                        timestamp_callback\n",
    "                       )\n",
    "\n",
    "# Dyna-Q+ specific:\n",
    "for i in np.ndindex(ModelState1.shape[:-1]):\n",
    "    ModelState1[i] = i[:-1]\n",
    "\n",
    "#sequence, test_sequence = itertools.tee(gen,2)\n",
    "RG = np.random.RandomState(12)\n",
    "DynaQPlus(Q, planning, ModelReward, ModelState1, iter(gen), n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,3000,100), DaynaQ_total_reward.TotalReward)\n",
    "plt.plot(range(0,3000,100), DaynaQPlus_total_reward.TotalReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.95\n",
    "n = 5\n",
    "TotalReward = 0.0\n",
    "Reward = [0.0]\n",
    "\n",
    "def callback(e,s,r):\n",
    "    global TotalReward\n",
    "    TotalReward += r\n",
    "    if s % 100 == 0: Reward.append(TotalReward)\n",
    "\n",
    "gen = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.4),\n",
    "                        lambda e: start_cell_1,\n",
    "                        lambda s,a: maze_getTransition(maze1, lambda: start_cell_1, s, a),\n",
    "                        0,\n",
    "                        3000,\n",
    "                        callback\n",
    "                       )\n",
    "sequence = iter(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DynaQ(Q, ModelReward, ModelState1, sequence, n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = np.transpose(np.nonzero((ModelState1 != UNOBSERVED).all(axis=-1)))\n",
    "len(observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = maze1\n",
    "getStartCell = lambda e=1: start_cell_1\n",
    "random_generator.seed(1)\n",
    "gen = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.1),\n",
    "                        getStartCell,\n",
    "                        lambda s,a: maze_getTransition(maze, getStartCell, s, a),\n",
    "                        1\n",
    "                       )\n",
    "sequence = iter(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DynaQ(Q, ModelReward, ModelState1, sequence, n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros(shape)\n",
    "maze = buildMaze(maze_shape, maze_obstacles_1, start_cell_1, target_cell_1)\n",
    "\n",
    "for seed_ in range(0,50):\n",
    "    random_generator.seed(seed_)\n",
    "    gen = SequenceGenerator(EpsilonGreedyPolicy(Q, 0.1),\n",
    "                    lambda e: start_cell,\n",
    "                    lambda s,a: maze_getTransition(maze, s, a),\n",
    "                    1,\n",
    "                    20000,\n",
    "                   )\n",
    "    sequence = iter(gen)\n",
    "    if len(list(sequence)) < 20000: break\n",
    "        \n",
    "seed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "start_cell = (3,0)\n",
    "target_cell = (5,8)\n",
    "Maze3 = buildMaze(maze_shape, maze_obstacles_3, start_cell, target_cell)\n",
    "\n",
    "class DynaMazeLearinig:\n",
    "    def __init__(self, shape):\n",
    "        self.Q = np.zeros(shape)\n",
    "        self.ModelReward = np.zeros(shape)\n",
    "        self.ModelState1 = np.full(shape+(2,),-1)\n",
    "        \n",
    "setup1 = DynaMazeLearinig(shape)\n",
    "setup1.Q[...,1] = 1.e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_num = np.zeros(shape=(50,), dtype=np.int32)\n",
    "\n",
    "def callback(e,s,r):\n",
    "    global steps_num\n",
    "    steps_num[e-1] = s\n",
    "\n",
    "random_generator.seed(3)\n",
    "gen = SequenceGenerator(EpsilonGreedyPolicy(setup1.Q, epsilon),\n",
    "                lambda e: start_cell,\n",
    "                lambda s,a: maze_getTransition(Maze3, s, a),\n",
    "                50,\n",
    "                0,\n",
    "                callback\n",
    "               )\n",
    "sequence = iter(gen)\n",
    "#len(list(sequence)), steps_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "RG.seed(1)\n",
    "\n",
    "DynaQ(setup1.Q, setup1.ModelReward, setup1.ModelState1, sequence, n, alpha, gamma, UNOBSERVED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Dyna Maze 3 times with n=0, n=5, n=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = []\n",
    "\n",
    "for n in [0,5,50]:\n",
    "    random_generator.seed(3)\n",
    "    setup1 = Learinig3(shape)\n",
    "    setup1.Q[...,1] = 1.e-9\n",
    "    \n",
    "    gen = SequenceGenerator(EpsilonGreedyPolicy(setup1.Q, epsilon),\n",
    "                    lambda e: start_cell,\n",
    "                    lambda s,a: maze_getTransition(Maze3, s, a),\n",
    "                    50,\n",
    "                    0,\n",
    "                    callback\n",
    "                   )\n",
    "    sequence = iter(gen)\n",
    "    \n",
    "    RG.seed(5)\n",
    "    DynaQPlus(setup1.Q, setup1.Q, setup1.ModelReward, setup1.ModelState1, sequence, n, alpha, gamma, UNOBSERVED)\n",
    "    rate.append([steps_num[j]-steps_num[j-1] for j in range(1,len(steps_num))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the number of steps in each episode (excluding the first one) for all the three runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rate[0])\n",
    "plt.plot(rate[1])\n",
    "plt.plot(rate[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = maze1\n",
    "f = plt.figure(num=None, figsize=tuple(reversed(maze.shape)), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.pcolor(maze, figure=f)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
