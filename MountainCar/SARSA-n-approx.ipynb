{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_approx(x,w):\n",
    "    return np.matmul(w,x)\n",
    "\n",
    "def lin_approx_grad(x,w):\n",
    "    q = lin_approx(x,w)\n",
    "    grad = np.repeat(np.transpose(x), q.shape[0], axis=0)\n",
    "    return (q,grad)\n",
    "\n",
    "def lin_sigmoid(x,w):\n",
    "    return 1.0/(1.0 + np.exp(-np.matmul(w,x)))\n",
    "\n",
    "def lin_sigmoid_grad(x,w):\n",
    "    q = lin_approx(x,w)\n",
    "    grad = np.matmul(q*(1.0-q),np.transpose(x))\n",
    "    return (q,grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_I = 4\n",
    "\n",
    "def SARSAnApprox(sequence, q_approx, q_grad, w, n, gamma, alpha):\n",
    "    gamma_powered = [gamma**i for i in range(0,n+1)]\n",
    "    sequence_iter = iter(sequence)\n",
    "    for episode_i in range(50000):  \n",
    "      try:\n",
    "        step = next(sequence_iter)\n",
    "        state, is_terminal, next_state, action, reward = step\n",
    "        T = sys.maxsize\n",
    "        t = 0\n",
    "        history = [step]\n",
    "\n",
    "        while True:\n",
    "            if t < T:\n",
    "                if is_terminal:\n",
    "                    T = t+1\n",
    "                    print(\"\\rEpisode length is {:7}; tau={:7}\".format(T,tau), end='', flush=True)\n",
    "                else:\n",
    "                    step = next(sequence_iter)\n",
    "                    state, is_terminal, next_state, action, next_reward = step\n",
    "                    history.append(step)\n",
    "\n",
    "            tau = t-n+1\n",
    "            if tau >= 0:\n",
    "                G = np.sum( [gamma_powered[j]*history[tau+j][REWARD_I]\n",
    "                             for j in range(0,min(n,T-tau))])\n",
    "                if tau+n < T:\n",
    "                    G = G + gamma_powered[n] * q_approx(state, action, w)\n",
    "                Stau, istrm, next_tau, Atau, Rtau = history[tau]\n",
    "                q,grad = q_grad(state, action, w)\n",
    "                w += alpha*(G - q)*grad\n",
    "\n",
    "            if tau == T-1: break\n",
    "            t += 1\n",
    "\n",
    "      except StopIteration:\n",
    "        print(\"\\nSequence terminated.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\nFinished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiling(value, low_bound, high_bound, pitch):\n",
    "    n = int((high_bound-low_bound)/pitch + 1)\n",
    "    i = int((value - low_bound)/pitch)\n",
    "    return [x <= i+1 and x >= i-1 for x in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.env.observation_space.shape[0]\n",
    "actions_num = env.env.action_space.n\n",
    "state = env.reset()\n",
    "next_state, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_getStartPosition():\n",
    "    return (np.random.rand(1)[0]*0.2-0.6, 0.0)\n",
    "\n",
    "REWARD = -1.0\n",
    "\n",
    "def mc_getTransition(position, velocity, push):\n",
    "    finished = False\n",
    "    v = np.clip(velocity + 0.001*push - 0.0025*np.cos(3.0*position), -0.07, 0.07)\n",
    "    p = position + v\n",
    "    if p <= -1.2:\n",
    "        p = -1.2\n",
    "        v = 0.0\n",
    "    elif p >= 0.5:\n",
    "        p = 0.5\n",
    "        finished = True\n",
    "    return (p, v, finished, REWARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "import SeqGen\n",
    "\n",
    "def get_features(pos,vel):\n",
    "    return np.float32([1.0] + tiling(pos, -1.2, 0.5, 0.2) + tiling(vel, -0.07, 0.06999, 0.02))\n",
    "\n",
    "features_dim = len(get_features(.6, 0.01))\n",
    "getFeatures = lambda k: get_features(k[0],k[1])\n",
    "ACTION_FEATURES = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "actions_dim = len(ACTION_FEATURES)\n",
    "\n",
    "class getActionValues:\n",
    "    def __init__(self, approx_func, w, actions_num):\n",
    "        self.approx_func = approx_func\n",
    "        self.w = w\n",
    "        self.actions_num = actions_num\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        q=[]\n",
    "        for action in range(self.actions_num):\n",
    "            q.append(self.approx_func(state, action, w))\n",
    "        return q\n",
    "\n",
    "def tiling_lin_approx(state, action, w):\n",
    "    x = np.concatenate((get_features(state[0],state[1]), ACTION_FEATURES[action]), axis=None)\n",
    "    return lin_approx(np.transpose(np.array([x])), w)\n",
    "\n",
    "def tiling_lin_approx_grad(state, action, w):\n",
    "    x = np.concatenate((get_features(state[0],state[1]), ACTION_FEATURES[action]), axis=None)\n",
    "    return lin_approx_grad(np.transpose(np.array([x])), w)\n",
    "\n",
    "w = (np.random.rand(1, features_dim + actions_dim) - 0.5) * 0.001\n",
    "# Need for SeqGen.EpsilonGreedyPolicy\n",
    "q_s_a_func = getActionValues(tiling_lin_approx, w, actions_dim)\n",
    "\n",
    "def getStateTransition(s,a):\n",
    "    p,v,f,r = mc_getTransition(s[0], s[1], a-1.0)\n",
    "    return (f, (p,v), r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = SequenceGeneratorPlus(SeqGen.EpsilonGreedyPolicy(q_s_a_func, 0.2), \n",
    "                                  mc_getStartPosition,\n",
    "                                  getStateTransition,\n",
    "                                  100,\n",
    "                                  episode_maxlen = 5500\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSAnApprox(sequence, tiling_lin_approx, tiling_lin_approx_grad, \\\n",
    "             w, 4, 0.95, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "n = 20\n",
    "\n",
    "gamma_powered = [gamma**i for i in range(0,n+1)]\n",
    "sequence_iter = iter(sequence)\n",
    "for episode_i in range(50):\n",
    "    try:\n",
    "        step = next(sequence_iter)\n",
    "        state, is_terminal, next_state, action, reward = step\n",
    "        T = sys.maxsize\n",
    "        t = 0\n",
    "        history = [step]\n",
    "\n",
    "        while True:\n",
    "            if t < T:\n",
    "                if is_terminal:\n",
    "                    T = t+1\n",
    "                    print(\"\\rEpisode length is {:7}; tau={:7}\".format(T,tau), end='', flush=True)\n",
    "                else:\n",
    "                    step = next(sequence_iter)\n",
    "                    state, is_terminal, next_state, action, next_reward = step\n",
    "                    history.append(step)\n",
    "\n",
    "            tau = t-n+1\n",
    "            if tau >= 0:\n",
    "                G = np.sum( [gamma_powered[j]*history[tau+j][REWARD_I]\n",
    "                             for j in range(0,min(n,T-tau))])\n",
    "                if tau+n < T:\n",
    "                    G = G + gamma_powered[n] * tiling_lin_approx(state, action, w)\n",
    "                Stau, istrm, next_tau, Atau, Rtau = history[tau]\n",
    "                q,grad = tiling_lin_approx_grad(Stau, Atau, w)\n",
    "                w += alpha*(G - q)*grad\n",
    "\n",
    "            if tau == T-1: break\n",
    "            t += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nSequence terminated.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFinished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "Stau, istrm, next_tau, Atau, Rtau = history[tau]\n",
    "q_s_a_func[Stau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=(-.2, -0.02)\n",
    "q_s_a_func[s], np.argmax(q_s_a_func[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=(.0001, 0.049)\n",
    "q_s_a_func[s], np.argmax(q_s_a_func[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGeneratorPlus:\n",
    "    def __init__(self, getAction, getStartState, getTransition, episode_imax=1, steps_max=0,\n",
    "                callBack=None, episode_maxlen=0):\n",
    "        self.episode_imax = episode_imax\n",
    "        self.get_action = getAction\n",
    "        self.get_start_state = getStartState\n",
    "        self.get_transition = getTransition\n",
    "        self.steps_max = steps_max\n",
    "        self.callback = callBack\n",
    "        self.episode_maxlen = episode_maxlen\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.episode_i=1\n",
    "        self.state = None\n",
    "        self.step_i = 0\n",
    "        self.episode_step = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.episode_imax > 0 and self.episode_i > self.episode_imax or \\\n",
    "            self.steps_max > 0 and self.step_i > self.steps_max:\n",
    "            raise StopIteration\n",
    "\n",
    "        if self.state == None:\n",
    "            self.state = self.get_start_state()\n",
    "            self.episode_step = 0\n",
    "\n",
    "        action = self.get_action(self.state)\n",
    "        keep_state = self.state\n",
    "        is_terminal, self.state, reward = self.get_transition(keep_state, action)\n",
    "        \n",
    "        if self.episode_maxlen > 0 and self.episode_step >= self.episode_maxlen:\n",
    "            is_terminal = True\n",
    "\n",
    "        if self.callback:\n",
    "            self.callback(self, keep_state, is_terminal, self.state, action, reward)\n",
    "\n",
    "        self.step_i += 1\n",
    "        self.episode_i += int(is_terminal)\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if is_terminal:\n",
    "            self.state = None\n",
    "\n",
    "        return keep_state, is_terminal, self.state, action, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.5, 0.1, 1.0]])\n",
    "np.repeat(a, 2, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
